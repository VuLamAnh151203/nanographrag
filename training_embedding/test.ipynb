{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open(\"/home/hungpv/projects/train_embedding/nanographrag/training_embedding/zalo_wiki_qa/train.json\", \"r\") as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open(\"/home/hungpv/projects/train_embedding/nanographrag/training_embedding/zalo_wiki_qa/test.json\", \"r\") as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate \n",
    "def Jaccard_Similarity(doc1, doc2): \n",
    "    \n",
    "    # List the unique words in a document\n",
    "    words_doc1 = set(doc1.lower().split()) \n",
    "    words_doc2 = set(doc2.lower().split())\n",
    "    \n",
    "    # Find the intersection of words list of doc1 & doc2\n",
    "    intersection = words_doc1.intersection(words_doc2)\n",
    "\n",
    "    # Find the union of words list of doc1 & doc2\n",
    "    union = words_doc1.union(words_doc2)\n",
    "        \n",
    "    # Calculate Jaccard similarity score \n",
    "    # using length of intersection set divided by length of union set\n",
    "    return float(len(intersection)) / len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hungpv/miniconda3/envs/embedding_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# translation model\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer_vi2en = AutoTokenizer.from_pretrained(\"vinai/vinai-translate-vi2en-v2\", src_lang=\"vi_VN\")\n",
    "model_vi2en = AutoModelForSeq2SeqLM.from_pretrained(\"vinai/vinai-translate-vi2en-v2\")\n",
    "device_vi2en = torch.device(\"cuda:0\")\n",
    "model_vi2en.to(device_vi2en)\n",
    "\n",
    "\n",
    "def translate_vi2en(vi_texts: str) -> str:\n",
    "    input_ids = tokenizer_vi2en(vi_texts, padding=True, return_tensors=\"pt\").to(device_vi2en)\n",
    "    output_ids = model_vi2en.generate(\n",
    "        **input_ids,\n",
    "        decoder_start_token_id=tokenizer_vi2en.lang_code_to_id[\"en_XX\"],\n",
    "        num_return_sequences=1,\n",
    "        num_beams=5,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    en_texts = tokenizer_vi2en.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    return en_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m english_translation = []\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m,\u001b[38;5;28mlen\u001b[39m(text_data), batch_size):\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     english_translation.extend(\u001b[43mtranslate_vi2en\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mtranslate_vi2en\u001b[39m\u001b[34m(vi_texts)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtranslate_vi2en\u001b[39m(vi_texts: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m     12\u001b[39m     input_ids = tokenizer_vi2en(vi_texts, padding=\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(device_vi2en)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     output_ids = \u001b[43mmodel_vi2en\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoder_start_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_vi2en\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlang_code_to_id\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43men_XX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     en_texts = tokenizer_vi2en.batch_decode(output_ids, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m en_texts\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/embedding_env/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/embedding_env/lib/python3.12/site-packages/transformers/generation/utils.py:2345\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[39m\n\u001b[32m   2338\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2339\u001b[39m         input_ids=input_ids,\n\u001b[32m   2340\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2341\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2342\u001b[39m         **model_kwargs,\n\u001b[32m   2343\u001b[39m     )\n\u001b[32m   2344\u001b[39m     \u001b[38;5;66;03m# 12. run beam sample\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2345\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2346\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2347\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2348\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2349\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2350\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2351\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2352\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2354\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode == GenerationMode.GROUP_BEAM_SEARCH:\n\u001b[32m   2355\u001b[39m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[32m   2356\u001b[39m     beam_scorer = BeamSearchScorer(\n\u001b[32m   2357\u001b[39m         batch_size=batch_size,\n\u001b[32m   2358\u001b[39m         num_beams=generation_config.num_beams,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2364\u001b[39m         max_length=generation_config.max_length,\n\u001b[32m   2365\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/embedding_env/lib/python3.12/site-packages/transformers/generation/utils.py:3751\u001b[39m, in \u001b[36mGenerationMixin._beam_search\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[39m\n\u001b[32m   3748\u001b[39m beam_indices = running_beam_indices.clone().detach()\n\u001b[32m   3750\u001b[39m \u001b[38;5;66;03m# 4. run the generation loop\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3751\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_has_unfinished_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthis_peer_finished\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   3752\u001b[39m     \u001b[38;5;66;03m# a. Forward current tokens, obtain the logits\u001b[39;00m\n\u001b[32m   3753\u001b[39m     flat_running_sequences = \u001b[38;5;28mself\u001b[39m._flatten_beam_dim(running_sequences[:, :, :cur_len])\n\u001b[32m   3754\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.prepare_inputs_for_generation(flat_running_sequences, **model_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/embedding_env/lib/python3.12/site-packages/transformers/generation/utils.py:2463\u001b[39m, in \u001b[36mGenerationMixin._has_unfinished_sequences\u001b[39m\u001b[34m(self, this_peer_finished, synced_gpus, device)\u001b[39m\n\u001b[32m   2460\u001b[39m         result.past_key_values = result.past_key_values.to_legacy_cache()\n\u001b[32m   2461\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m-> \u001b[39m\u001b[32m2463\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_has_unfinished_sequences\u001b[39m(\u001b[38;5;28mself\u001b[39m, this_peer_finished: \u001b[38;5;28mbool\u001b[39m, synced_gpus: \u001b[38;5;28mbool\u001b[39m, device: torch.device) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m   2464\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2465\u001b[39m \u001b[33;03m    Returns whether there are still unfinished sequences in the device. The existence of unfinished sequences is\u001b[39;00m\n\u001b[32m   2466\u001b[39m \u001b[33;03m    fed through `this_peer_finished`. ZeRO stage 3-friendly.\u001b[39;00m\n\u001b[32m   2467\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   2468\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m synced_gpus:\n\u001b[32m   2469\u001b[39m         \u001b[38;5;66;03m# Under synced_gpus the `forward` call must continue until all gpus complete their sequence.\u001b[39;00m\n\u001b[32m   2470\u001b[39m         \u001b[38;5;66;03m# The following logic allows an early break if all peers finished generating their sequence\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "text_data = [sample[\"text\"] for sample in train_data]\n",
    "english_translation = []\n",
    "for i in range(0,len(text_data), batch_size):\n",
    "    english_translation.extend(translate_vi2en(text_data[i: i + batch_size]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embedding_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
