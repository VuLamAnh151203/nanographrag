2025-03-29 13:52:17 - Use pytorch device_name: cpu
2025-03-29 13:52:17 - Load pretrained SentenceTransformer: /home/vulamanh/Documents/nanographrag/models/bge_m3
2025-03-29 13:52:17 - Teacher model: SentenceTransformer(
  (0): Transformer({'max_seq_length': 8192, 'do_lower_case': False}) with Transformer model: XLMRobertaModel 
  (1): Pooling({'word_embedding_dimension': 1024, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
)
2025-03-29 13:52:17 - Use pytorch device_name: cpu
2025-03-29 13:52:17 - Load pretrained SentenceTransformer: /home/vulamanh/Documents/nanographrag/models/bge_m3
2025-03-29 13:52:18 - Student model: SentenceTransformer(
  (0): Transformer({'max_seq_length': 32, 'do_lower_case': False}) with Transformer model: XLMRobertaModel 
  (1): Pooling({'word_embedding_dimension': 1024, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
)
2025-03-29 13:52:24 - Could not load dataset None/en-vi dev split, splitting 1k samples from train
2025-03-29 13:52:24 - DatasetDict({
    en-vi: Dataset({
        features: ['english', 'non_english'],
        num_rows: 2
    })
})
2025-03-29 13:52:32 - Prepared datasets for training:
2025-03-29 13:52:32 - Creating evaluators for en-vi
